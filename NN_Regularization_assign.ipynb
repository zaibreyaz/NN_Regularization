{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Understanding Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1. What is regularization in the context of deep learning? Why is it important?\n",
    "    Ans: Regularization in deep learning refers to techniques that prevent overfitting by adding extra constraints or penalties to the model during training. \n",
    "         It is essential because it helps improve generalization, reducing the risk of the model memorizing the training data and performing poorly on unseen data.\n",
    " \n",
    "2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff.\n",
    "    Ans: The bias-variance tradeoff refers to the balance between a model's ability to fit training data (low bias) and generalize to unseen data (low variance).\n",
    "         Regularization helps address this tradeoff by adding penalties to the model's complexity during training. It reduces overfitting (high variance) by favoring simpler models (higher bias) that generalize better.\n",
    "\n",
    "3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model? \n",
    "    Ans:  L1 regularization adds a penalty proportional to the absolute value of weights, promoting sparsity. \n",
    "          L2 regularization adds a penalty proportional to the square of weights, encouraging smaller but non-zero weights. \n",
    "          L1 can lead to more sparse models, while L2 encourages smaller weights without making them exactly zero.\n",
    "\n",
    "4. Discuss the role of regularization in preventing overfitting and improving the generalization  of deep learning models. \n",
    "    Ans: Regularization prevents overfitting in deep learning models by adding constraints or penalties during training. \n",
    "         It discourages the model from becoming too complex and memorizing the training data. By favoring simpler models, regularization improves generalization, \n",
    "         allowing the model to perform better on unseen data and make more accurate predictions.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference. \n",
    "    Ans: Dropout regularization randomly deactivates (sets to zero) some neurons during training. This prevents neurons from relying too much on others, reducing overfitting.\n",
    "         During inference, neurons are activated with scaled weights to compensate for the dropout. Dropout can slow training, but it helps improve generalization and model performance.\n",
    "\n",
    "6. Describe the concept of Early Stopping as a form of regularization. How does it help prevent overfitting during the training process? \n",
    "    Ans: Early stopping is a form of regularization that stops training when the model's performance on a validation set starts to degrade. \n",
    "         By preventing the model from training for too long, it avoids overfitting. It ensures the model retains its ability to generalize well and perform better on unseen data.\n",
    "\n",
    "7. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting? \n",
    "    Ans: Batch Normalization is a technique that normalizes the activations in each layer of a neural network during training. \n",
    "         It helps prevent overfitting by reducing internal covariate shift, making training more stable. This regularization improves generalization and allows for faster convergence during training.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Applying Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "\n",
    "def model(dropout_rate):\n",
    "    # loading dataset\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
    "    X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "    y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "    # scale the test set as well\n",
    "    X_test = X_test / 255.\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Flatten(input_shape=[28, 28], name=\"inputLayer\"))\n",
    "    model.add(Dense(300, activation='relu', name=\"hiddenLayer1\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(100, activation='relu', name=\"hiddenLayer2\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation=\"softmax\", name=\"outputLayer\"))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, epochs=25,\n",
    "                    validation_data=(X_valid, y_valid), batch_size=1000)\n",
    "    \n",
    "    return max(model.history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "55/55 [==============================] - 3s 29ms/step - loss: 0.7251 - accuracy: 0.7874 - val_loss: 0.2522 - val_accuracy: 0.9288\n",
      "Epoch 2/25\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.2731 - accuracy: 0.9198 - val_loss: 0.1747 - val_accuracy: 0.9506\n",
      "Epoch 3/25\n",
      "55/55 [==============================] - 1s 27ms/step - loss: 0.2015 - accuracy: 0.9413 - val_loss: 0.1374 - val_accuracy: 0.9622\n",
      "Epoch 4/25\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.1628 - accuracy: 0.9526 - val_loss: 0.1156 - val_accuracy: 0.9678\n",
      "Epoch 5/25\n",
      "55/55 [==============================] - 1s 26ms/step - loss: 0.1391 - accuracy: 0.9589 - val_loss: 0.1024 - val_accuracy: 0.9708\n",
      "Epoch 6/25\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.1142 - accuracy: 0.9666 - val_loss: 0.0893 - val_accuracy: 0.9736\n",
      "Epoch 7/25\n",
      "55/55 [==============================] - 1s 26ms/step - loss: 0.1022 - accuracy: 0.9695 - val_loss: 0.0824 - val_accuracy: 0.9758\n",
      "Epoch 8/25\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.0890 - accuracy: 0.9733 - val_loss: 0.0753 - val_accuracy: 0.9774\n",
      "Epoch 9/25\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.0796 - accuracy: 0.9760 - val_loss: 0.0710 - val_accuracy: 0.9794\n",
      "Epoch 10/25\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 0.0704 - accuracy: 0.9790 - val_loss: 0.0688 - val_accuracy: 0.9796\n",
      "Epoch 11/25\n",
      "55/55 [==============================] - 2s 41ms/step - loss: 0.0641 - accuracy: 0.9808 - val_loss: 0.0666 - val_accuracy: 0.9794\n",
      "Epoch 12/25\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 0.0589 - accuracy: 0.9814 - val_loss: 0.0639 - val_accuracy: 0.9808\n",
      "Epoch 13/25\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 0.0535 - accuracy: 0.9837 - val_loss: 0.0638 - val_accuracy: 0.9814\n",
      "Epoch 14/25\n",
      "55/55 [==============================] - 2s 40ms/step - loss: 0.0483 - accuracy: 0.9846 - val_loss: 0.0612 - val_accuracy: 0.9812\n",
      "Epoch 15/25\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.0453 - accuracy: 0.9861 - val_loss: 0.0634 - val_accuracy: 0.9810\n",
      "Epoch 16/25\n",
      "55/55 [==============================] - 2s 40ms/step - loss: 0.0409 - accuracy: 0.9872 - val_loss: 0.0623 - val_accuracy: 0.9804\n",
      "Epoch 17/25\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.0390 - accuracy: 0.9881 - val_loss: 0.0598 - val_accuracy: 0.9824\n",
      "Epoch 18/25\n",
      "55/55 [==============================] - 2s 41ms/step - loss: 0.0348 - accuracy: 0.9891 - val_loss: 0.0587 - val_accuracy: 0.9830\n",
      "Epoch 19/25\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.0314 - accuracy: 0.9905 - val_loss: 0.0583 - val_accuracy: 0.9838\n",
      "Epoch 20/25\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.0292 - accuracy: 0.9909 - val_loss: 0.0609 - val_accuracy: 0.9816\n",
      "Epoch 21/25\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.0281 - accuracy: 0.9911 - val_loss: 0.0571 - val_accuracy: 0.9834\n",
      "Epoch 22/25\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.0282 - accuracy: 0.9915 - val_loss: 0.0597 - val_accuracy: 0.9828\n",
      "Epoch 23/25\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.0241 - accuracy: 0.9928 - val_loss: 0.0601 - val_accuracy: 0.9826\n",
      "Epoch 24/25\n",
      "55/55 [==============================] - 2s 40ms/step - loss: 0.0228 - accuracy: 0.9930 - val_loss: 0.0591 - val_accuracy: 0.9834\n",
      "Epoch 25/25\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.0597 - val_accuracy: 0.9838\n",
      "Epoch 1/25\n",
      "55/55 [==============================] - 4s 38ms/step - loss: 0.6203 - accuracy: 0.8385 - val_loss: 0.2493 - val_accuracy: 0.9276\n",
      "Epoch 2/25\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.2231 - accuracy: 0.9359 - val_loss: 0.1749 - val_accuracy: 0.9508\n",
      "Epoch 3/25\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 0.1655 - accuracy: 0.9519 - val_loss: 0.1392 - val_accuracy: 0.9586\n",
      "Epoch 4/25\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 0.1302 - accuracy: 0.9622 - val_loss: 0.1241 - val_accuracy: 0.9636\n",
      "Epoch 5/25\n",
      "55/55 [==============================] - 2s 33ms/step - loss: 0.1073 - accuracy: 0.9697 - val_loss: 0.1046 - val_accuracy: 0.9710\n",
      "Epoch 6/25\n",
      "55/55 [==============================] - 2s 36ms/step - loss: 0.0864 - accuracy: 0.9758 - val_loss: 0.1005 - val_accuracy: 0.9682\n",
      "Epoch 7/25\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 0.0744 - accuracy: 0.9792 - val_loss: 0.0892 - val_accuracy: 0.9728\n",
      "Epoch 8/25\n",
      "55/55 [==============================] - 2s 35ms/step - loss: 0.0621 - accuracy: 0.9825 - val_loss: 0.0808 - val_accuracy: 0.9754\n",
      "Epoch 9/25\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 0.0525 - accuracy: 0.9855 - val_loss: 0.0771 - val_accuracy: 0.9766\n",
      "Epoch 10/25\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.0447 - accuracy: 0.9877 - val_loss: 0.0790 - val_accuracy: 0.9762\n",
      "Epoch 11/25\n",
      "55/55 [==============================] - 2s 34ms/step - loss: 0.0382 - accuracy: 0.9899 - val_loss: 0.0708 - val_accuracy: 0.9778\n",
      "Epoch 12/25\n",
      "55/55 [==============================] - 2s 36ms/step - loss: 0.0327 - accuracy: 0.9913 - val_loss: 0.0690 - val_accuracy: 0.9772\n",
      "Epoch 13/25\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 0.0278 - accuracy: 0.9927 - val_loss: 0.0696 - val_accuracy: 0.9786\n",
      "Epoch 14/25\n",
      "55/55 [==============================] - 2s 41ms/step - loss: 0.0246 - accuracy: 0.9943 - val_loss: 0.0706 - val_accuracy: 0.9768\n",
      "Epoch 15/25\n",
      "55/55 [==============================] - 2s 41ms/step - loss: 0.0200 - accuracy: 0.9957 - val_loss: 0.0664 - val_accuracy: 0.9784\n",
      "Epoch 16/25\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.0167 - accuracy: 0.9965 - val_loss: 0.0684 - val_accuracy: 0.9798\n",
      "Epoch 17/25\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 0.0145 - accuracy: 0.9975 - val_loss: 0.0678 - val_accuracy: 0.9792\n",
      "Epoch 18/25\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.0125 - accuracy: 0.9979 - val_loss: 0.0697 - val_accuracy: 0.9780\n",
      "Epoch 19/25\n",
      "55/55 [==============================] - 2s 35ms/step - loss: 0.0105 - accuracy: 0.9986 - val_loss: 0.0683 - val_accuracy: 0.9798\n",
      "Epoch 20/25\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0704 - val_accuracy: 0.9790\n",
      "Epoch 21/25\n",
      "55/55 [==============================] - 2s 41ms/step - loss: 0.0078 - accuracy: 0.9992 - val_loss: 0.0698 - val_accuracy: 0.9796\n",
      "Epoch 22/25\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.0723 - val_accuracy: 0.9794\n",
      "Epoch 23/25\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 0.0751 - val_accuracy: 0.9782\n",
      "Epoch 24/25\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 0.0715 - val_accuracy: 0.9796\n",
      "Epoch 25/25\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.0041 - accuracy: 0.9999 - val_loss: 0.0741 - val_accuracy: 0.9786\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model Performance With Dropout Layer : 0.9837999939918518\n",
      "Model Performance Without Dropout Layer : 0.9797999858856201\n"
     ]
    }
   ],
   "source": [
    "# 8. Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate its impact on model performance and compare it with a model without Dropout. \n",
    "\n",
    "model_performance_with_Dropout_Layer = model(dropout_rate=0.2)\n",
    "model_performance_without_Dropout_Layer = model(dropout_rate=0)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(f\"Model Performance With Dropout Layer : {model_performance_with_Dropout_Layer}\")\n",
    "print(f\"Model Performance Without Dropout Layer : {model_performance_without_Dropout_Layer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see in above out that using Dropout Layer out model performance increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"9. Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a given deep learning task. \n",
    "    Ans: When choosing a regularization technique, consider the task complexity and data size. L1 and L2 regularization penalize large weights, \n",
    "         while Dropout deactivates neurons. Each technique affects model training and inference differently. \n",
    "         Experimentation is key to finding the most suitable regularization method, balancing performance and computational cost.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
